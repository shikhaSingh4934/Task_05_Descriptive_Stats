# Task_05_Descriptive_Stats

## 📘 Project Overview

This research task explores the ability of large language models (LLMs) like ChatGPT to interpret and reason over structured sports datasets using natural language prompts. I used the Syracuse University (SU) Women’s Lacrosse dataset from a past season to generate statistics, ask questions, and evaluate model responses.

The goal is not just to get correct answers, but to experiment with prompt engineering, define meaningful metrics (e.g., “most improved player”), and document how well the model performs.

---

## 🏁 Objectives

- Clean and summarize the SU Women’s Lacrosse dataset
- Ask both simple and complex natural language questions to ChatGPT
- Evaluate the correctness of the answers using Python scripts
- Log all responses and failures for analysis
- Generate visual insights where relevant

---

## 📁 Folder Structure

```sh
Task_05_Descriptive_Stats/
├── prompts/ # Natural language questions, logs, and failures
├── scripts/ # Python scripts to summarize and validate data
├── README.md # This file
├── requirements.txt # Python dependencies
└── .gitignore # Files to exclude from version control
```