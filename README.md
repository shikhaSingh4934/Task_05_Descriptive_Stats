# Task_05_Descriptive_Stats

## ğŸ“˜ Project Overview

This research task explores the ability of large language models (LLMs) like ChatGPT to interpret and reason over structured sports datasets using natural language prompts. I used the Syracuse University (SU) Womenâ€™s Lacrosse dataset from a past season to generate statistics, ask questions, and evaluate model responses.

The goal is not just to get correct answers, but to experiment with prompt engineering, define meaningful metrics (e.g., â€œmost improved playerâ€), and document how well the model performs.

---

## ğŸ Objectives

- Clean and summarize the SU Womenâ€™s Lacrosse dataset
- Ask both simple and complex natural language questions to ChatGPT
- Evaluate the correctness of the answers using Python scripts
- Log all responses and failures for analysis
- Generate visual insights where relevant

---

## ğŸ“ Folder Structure

```sh
Task_05_Descriptive_Stats/
â”œâ”€â”€ prompts/ # Natural language questions, logs, and failures
â”œâ”€â”€ scripts/ # Python scripts to summarize and validate data
â”œâ”€â”€ README.md # This file
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ .gitignore # Files to exclude from version control
```